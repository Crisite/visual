import numpy as np
import pandas as pd
import jieba
import time
import requests
import json

from IPython.display import Image  # ç”¨äºåœ¨jupyter labä¸­æ˜¾ç¤ºæœ¬åœ°å›¾ç‰‡
from pyecharts.charts import Pie, Bar, Line, Page
from pyecharts import options as opts
from pyecharts.globals import SymbolType
from stylecloud import gen_stylecloud
# from wordcloud import WordCloud

df_douban = pd.read_csv('./data/Mojito6.12.csv')
# æŸ¥çœ‹ä¿¡æ¯
# print(df_douban.head().to_string())
df_douban.info()

# æŸ¥çœ‹é‡å¤æ¡æ•°
# print(df_douban.duplicated().sum())

# æ–°å»ºä¸€åˆ—starè®°å½•åˆ†æ•°
df_douban['star'] = df_douban.rating_num.str.extract(r'(\d)')

# åˆ é™¤ rating_numã€user_urlã€comment_time
df_douban = df_douban.drop(['rating_num', 'comment_time', 'user_url'], axis=1)
# print(df_douban.head(20).to_string())

# å¤„ç†å¼‚å¸¸å€¼ bæ›¿æ¢a
df_douban['content'] = df_douban.content.replace('ğŸ¤¨', 'å¾®ç¬‘')

# è¾“å…¥API Keyå’ŒSecret Key
# ak = 'iBrqRI4BQunrDH7Bi1060bBG'
# sk = 'IkBdZFZQ2kBKVp3i1iXlDVcZzPQdGNmP'
#
# host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={}&client_secret={}'.format(ak,
#                                                                                                                      sk)
#
# # å‘èµ·è¯·æ±‚
# r = requests.post(host)
#
# # è·å–token
# token = r.json()['access_token']
#
# def get_sentiment_score(text):
#     """
#     è¾“å…¥æ–‡æœ¬ï¼Œè¿”å›æƒ…æ„Ÿå€¾å‘å¾—åˆ†
#     """
#     url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?charset=UTF-8&access_token={}'.format(token)
#     data = {
#         'text': text
#     }
#     data = json.dumps(data)  #å­—å…¸-å­—ç¬¦ä¸²
#     # å‘èµ·è¯·æ±‚
#     try:
#         res = requests.post(url, data=data, timeout=3)
#         items_score = res.json()['items']
#     except Exception as e:
#         time.sleep(3)
#         res = requests.post(url, data=data, timeout=3)
#         items_score = res.json()['items']
#     return items_score


# è°ƒç”¨å‡½æ•°åˆ†ææ¯æ¡è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘

# score_list = []
#
# step = 0
# for i in df_douban['content']:
#     score = get_sentiment_score(i)
#     # æ‰“å°è¿›åº¦
#     step += 1
#     print('æˆ‘æ­£åœ¨è·å–ç¬¬{}ä¸ªè¯„åˆ†'.format(step), end='\n')
#     score_list.append(score)
# df = pd.DataFrame(score_list)
# df.to_csv(R'./data/pingfen.csv', encoding="utf-8")


""""
è°ƒç”¨apiå¤„ç†æ•°æ®å¤ªæ…¢ï¼Œå°†å¤„ç†å®Œçš„æ•°æ®ä¿å­˜åœ¨pingfen.csvä¸­
ç›´æ¥è¯»å–csvæ–‡ä»¶ä¸­çš„æ•°æ®å³å¯
"""

df = pd.read_csv('./data/pingfen.csv', names=['key', 'values'], index_col=None)
# print('+++++++++')
# print(df.loc[0])
# print('+++++++++')
# df.drop(index=[0])
df.dropna(subset=['key', 'values'],
          axis=0,  # axis=0è¡¨ç¤ºåˆ é™¤è¡Œï¼›
          how='any',  # how=anyè¡¨ç¤ºè‹¥åˆ—nameã€ageä¸­ï¼Œä»»æ„ä¸€ä¸ªå‡ºç°ç©ºå€¼ï¼Œå°±åˆ æ‰è¯¥è¡Œ
          inplace=True  # inplace=Trueè¡¨ç¤ºåœ¨åŸdfä¸Šè¿›è¡Œä¿®æ”¹ï¼›
          )
# df.columns=['key','values']
# df_dict = dict(zip(df['key'],df['value']))
# print(type(df.loc[1]))
# df.info()
# print(df_dict)
# print(df['values'])
# print(df['values'].values.tolist())
# print(type(df_list))
# for i in range()
# print(df.head())
positive_prob = []
negative_prob = []

for i in range(1, 499):
    # æå–æ­£è´Ÿæ¦‚ç‡
    positive_prob.append(eval(df.loc[i, 'values'])['positive_prob'])
    negative_prob.append(eval(df.loc[i, 'values'])['negative_prob'])

# print(score_list)
# print(score_list[1]['positive_prob'])


# positive_prob = [i[0]['positive_prob'] for i in score_list]
# negative_prob = [i[0]['negative_prob'] for i in score_list]

df_douban['positive_prob'] = positive_prob
df_douban['negative_prob'] = negative_prob

# æ­£è´Ÿå‘
df_douban['label'] = ['æ­£å‘' if i > 0.5 else 'è´Ÿå‘' for i in df_douban.positive_prob]
print(df_douban.head())

# å„æ˜Ÿçº§æ•°é‡
star_num = df_douban.star.value_counts()
star_num = star_num.sort_index()

#   å„è¯„è®ºæ˜Ÿçº§å æ¯”Pieå›¾
data_pair = [list(z) for z in zip([i + 'æ˜Ÿ' for i in star_num.index], star_num.values.tolist())]
pie1 = (
    Pie(init_opts=opts.InitOpts(width='1350px', height='750px'))
        .add('', data_pair, radius=['35%', '60%'])
        .set_global_opts(
        title_opts=opts.TitleOpts(title='è±†ç“£çŸ­è¯„è¯„åˆ†å æ¯”'),
        legend_opts=opts.LegendOpts(orient='vertical', pos_top='15%', pos_left='2%')
    )
        .set_series_opts(label_opts=opts.LabelOpts(formatter='{b}:{d}%'))
    # .render()
)


#
def get_cut_words(content_series):
    # è¯»å…¥åœç”¨è¯è¡¨
    stop_words = []

    with open(r"./data/å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt", 'r', encoding='gb18030') as f:
        lines = f.readlines()
        for line in lines:
            stop_words.append(line.strip())

    # æ·»åŠ å…³é”®è¯
    my_words = ['å‘¨æ°ä¼¦', 'ä¸€é¦–æ­Œ']
    for i in my_words:
        jieba.add_word(i)

    #     è‡ªå®šä¹‰åœç”¨è¯
    my_stop_words = ['æ­Œæœ‰', 'çœŸçš„', 'è¿™é¦–', 'ä¸€é¦–', 'ä¸€ç‚¹',
                     'åæ­£', 'ä¸€æ®µ', 'ä¸€å¥', 'é¦–æ­Œ']
    stop_words.extend(my_stop_words)

    # åˆ†è¯
    word_num = jieba.lcut(content_series.str.cat(sep='ã€‚'), cut_all=False)

    # æ¡ä»¶ç­›é€‰
    word_num_selected = [i for i in word_num if i not in stop_words and len(i) >= 2]

    return word_num_selected


text1 = get_cut_words(content_series=df_douban[(df_douban.star == '4') | (df_douban.star == '5')]['content'])
# print(text1[:5])
# print(text1)

# ç»˜åˆ¶è¯äº‘å›¾
gen_stylecloud(
            text=' '.join(text1),
            max_words=1000,
            collocations=False,
            font_path=r'./data/ç»å…¸ç»¼è‰ºä½“ç®€.TTF',
            icon_name='fas fa-thumbs-up',
            size=612,
            output_name='è±†ç“£æ­£å‘è¯„åˆ†è¯äº‘å›¾.png'
              )
Image(filename='è±†ç“£æ­£å‘è¯„åˆ†è¯äº‘å›¾.png')
# ç»˜åˆ¶è¯äº‘
# class WordCloud(
#     # åˆå§‹åŒ–é…ç½®é¡¹ï¼Œå‚è€ƒ `global_options.InitOpts`
#     init_opts: opts.InitOpts = opts.InitOpts()
# )
# è¯äº‘å›¾ https://blog.csdn.net/weixin_36279318/article/details/79278403
# str = ''.join(text1)
# print(str)
# my_cloud = WordCloud(font_path='./data/ç»å…¸ç»¼è‰ºä½“ç®€.TTF',max_words=2000,width=1000, height=700, background_color='white',).generate(str)
# image = my_cloud.to_image()
# image.show()
